name: Docker E2E Tests

on:
  workflow_dispatch: # Manual trigger
  push:
    branches: [main] # Trigger on merge to main

env:
  IMAGE_NAME: looplia:test

jobs:
  docker-e2e:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Install dependencies
        run: bun install --frozen-lockfile

      - name: Build project
        run: bun run build

      - name: Build Docker image
        run: docker build -t $IMAGE_NAME .

      - name: Create test workspace
        run: mkdir -p test-workspace

      - name: Run summarize command
        env:
          CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
        run: |
          # Use docker create + start + cp to avoid volume mount conflicts
          # (bootstrap deletes ~/.looplia which fails if it's a mounted volume)
          CONTAINER_ID=$(docker create \
            -e CLAUDE_CODE_OAUTH_TOKEN="$CLAUDE_CODE_OAUTH_TOKEN" \
            -v "$(pwd)/examples:/examples:ro" \
            $IMAGE_NAME \
            summarize --file /examples/ai-healthcare.md \
            --output /home/looplia/.looplia/summary.json)

          docker start -a "$CONTAINER_ID"
          docker cp "$CONTAINER_ID:/home/looplia/.looplia/." test-workspace/
          docker rm "$CONTAINER_ID"

      - name: Validate summary schema
        run: |
          echo "Validating summary schema..."
          jq -e '.headline and .tldr and .bullets and .tags' test-workspace/summary.json
          echo "Summary schema valid"

      - name: Validate summary quality
        run: |
          echo "Checking summary quality metrics..."
          TLDR_WORDS=$(jq -r '.tldr' test-workspace/summary.json | wc -w)
          BULLET_COUNT=$(jq '.bullets | length' test-workspace/summary.json)
          TAG_COUNT=$(jq '.tags | length' test-workspace/summary.json)

          echo "TLDR words: $TLDR_WORDS (expected: 30-200)"
          echo "Bullet count: $BULLET_COUNT (expected: 3-7)"
          echo "Tag count: $TAG_COUNT (expected: >= 3)"

          # Validate minimum quality thresholds
          [ "$BULLET_COUNT" -ge 3 ] || (echo "Bullet count too low" && exit 1)
          [ "$TAG_COUNT" -ge 3 ] || (echo "Tag count too low" && exit 1)

          echo "Summary quality acceptable"

      - name: Run kit command
        env:
          CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
        run: |
          # Use docker create + start + cp to avoid volume mount conflicts
          CONTAINER_ID=$(docker create \
            -e CLAUDE_CODE_OAUTH_TOKEN="$CLAUDE_CODE_OAUTH_TOKEN" \
            -v "$(pwd)/examples:/examples:ro" \
            $IMAGE_NAME \
            kit --file /examples/ai-healthcare.md \
            --topics "ai,healthcare,technology" \
            --tone "expert" \
            --output /home/looplia/.looplia/kit.json)

          docker start -a "$CONTAINER_ID"
          docker cp "$CONTAINER_ID:/home/looplia/.looplia/." test-workspace/
          docker rm "$CONTAINER_ID"

      - name: Validate kit schema
        run: |
          echo "Validating kit schema..."
          jq -e '.contentId and .summary and .ideas and .suggestedOutline' test-workspace/kit.json
          echo "Kit schema valid"

      - name: Validate kit quality
        run: |
          echo "Checking kit quality metrics..."
          HOOK_COUNT=$(jq '.ideas.hooks | length' test-workspace/kit.json)
          ANGLE_COUNT=$(jq '.ideas.angles | length' test-workspace/kit.json)
          QUESTION_COUNT=$(jq '.ideas.questions | length' test-workspace/kit.json)
          SECTION_COUNT=$(jq '.suggestedOutline | length' test-workspace/kit.json)

          echo "Hook count: $HOOK_COUNT (expected: >= 2)"
          echo "Angle count: $ANGLE_COUNT (expected: >= 2)"
          echo "Question count: $QUESTION_COUNT (expected: >= 2)"
          echo "Outline sections: $SECTION_COUNT (expected: >= 3)"

          # Validate minimum quality thresholds
          [ "$HOOK_COUNT" -ge 2 ] || (echo "Hook count too low" && exit 1)
          [ "$SECTION_COUNT" -ge 3 ] || (echo "Outline section count too low" && exit 1)

          echo "Kit quality acceptable"

      # Test 3: VTT Caption Summarization
      - name: Run summarize on VTT caption
        env:
          CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
        run: |
          mkdir -p test-workspace/vtt-test
          CONTAINER_ID=$(docker create \
            -e CLAUDE_CODE_OAUTH_TOKEN="$CLAUDE_CODE_OAUTH_TOKEN" \
            -v "$(pwd)/examples:/examples:ro" \
            $IMAGE_NAME \
            summarize --file /examples/youtube/Anthropics/captions/EvtPBaaykdo.en.vtt)

          docker start -a "$CONTAINER_ID"
          docker cp "$CONTAINER_ID:/home/looplia/.looplia/." test-workspace/vtt-test/
          docker rm "$CONTAINER_ID"

      - name: Validate VTT summary
        run: |
          echo "Validating VTT summary..."
          # Find session folder and validate summary.json
          SESSION_DIR=$(find test-workspace/vtt-test/contentItem -maxdepth 1 -type d ! -name contentItem | head -1)
          echo "Session folder: $SESSION_DIR"
          jq -e '.headline and .tldr and .bullets and .tags' "$SESSION_DIR/summary.json"
          DETECTED=$(jq -r '.detectedSource // "unknown"' "$SESSION_DIR/summary.json")
          echo "Detected source type: $DETECTED"
          echo "VTT summary valid"

      # Test 4: SRT Transcript Summarization
      - name: Run summarize on SRT transcript
        env:
          CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
        run: |
          mkdir -p test-workspace/srt-test
          CONTAINER_ID=$(docker create \
            -e CLAUDE_CODE_OAUTH_TOKEN="$CLAUDE_CODE_OAUTH_TOKEN" \
            -v "$(pwd)/examples:/examples:ro" \
            $IMAGE_NAME \
            summarize --file /examples/youtube/Anthropics/transcripts/CBneTpXF1CQ.srt)

          docker start -a "$CONTAINER_ID"
          docker cp "$CONTAINER_ID:/home/looplia/.looplia/." test-workspace/srt-test/
          docker rm "$CONTAINER_ID"

      - name: Validate SRT summary
        run: |
          echo "Validating SRT summary..."
          SESSION_DIR=$(find test-workspace/srt-test/contentItem -maxdepth 1 -type d ! -name contentItem | head -1)
          echo "Session folder: $SESSION_DIR"
          jq -e '.headline and .tldr and .bullets and .tags' "$SESSION_DIR/summary.json"
          DETECTED=$(jq -r '.detectedSource // "unknown"' "$SESSION_DIR/summary.json")
          echo "Detected source type: $DETECTED"
          echo "SRT summary valid"

      # Test 5: JSON Transcript Summarization
      - name: Run summarize on JSON transcript
        env:
          CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
        run: |
          mkdir -p test-workspace/json-test
          CONTAINER_ID=$(docker create \
            -e CLAUDE_CODE_OAUTH_TOKEN="$CLAUDE_CODE_OAUTH_TOKEN" \
            -v "$(pwd)/examples:/examples:ro" \
            $IMAGE_NAME \
            summarize --file /examples/youtube/Anthropics/transcripts/CBneTpXF1CQ.json)

          docker start -a "$CONTAINER_ID"
          docker cp "$CONTAINER_ID:/home/looplia/.looplia/." test-workspace/json-test/
          docker rm "$CONTAINER_ID"

      - name: Validate JSON transcript summary
        run: |
          echo "Validating JSON transcript summary..."
          SESSION_DIR=$(find test-workspace/json-test/contentItem -maxdepth 1 -type d ! -name contentItem | head -1)
          echo "Session folder: $SESSION_DIR"
          jq -e '.headline and .tldr and .bullets and .tags' "$SESSION_DIR/summary.json"
          DETECTED=$(jq -r '.detectedSource // "unknown"' "$SESSION_DIR/summary.json")
          echo "Detected source type: $DETECTED"
          echo "JSON transcript summary valid"

      # Test 6: Kit with SRT Transcript
      - name: Run kit on SRT transcript
        env:
          CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
        run: |
          mkdir -p test-workspace/kit-srt-test
          CONTAINER_ID=$(docker create \
            -e CLAUDE_CODE_OAUTH_TOKEN="$CLAUDE_CODE_OAUTH_TOKEN" \
            -v "$(pwd)/examples:/examples:ro" \
            $IMAGE_NAME \
            kit --file /examples/youtube/Anthropics/transcripts/CBneTpXF1CQ.srt \
            --topics "coding,claude,automation" \
            --tone "expert")

          docker start -a "$CONTAINER_ID"
          docker cp "$CONTAINER_ID:/home/looplia/.looplia/." test-workspace/kit-srt-test/
          docker rm "$CONTAINER_ID"

      - name: Validate SRT kit
        run: |
          echo "Validating SRT kit..."
          SESSION_DIR=$(find test-workspace/kit-srt-test/contentItem -maxdepth 1 -type d ! -name contentItem | head -1)
          echo "Session folder: $SESSION_DIR"
          jq -e '.contentId and .summary and .ideas and .suggestedOutline' "$SESSION_DIR/writing-kit.json"
          HOOK_COUNT=$(jq '.ideas.hooks | length' "$SESSION_DIR/writing-kit.json")
          SECTION_COUNT=$(jq '.suggestedOutline | length' "$SESSION_DIR/writing-kit.json")
          echo "Hook count: $HOOK_COUNT"
          echo "Outline sections: $SECTION_COUNT"
          [ "$HOOK_COUNT" -ge 2 ] || (echo "Hook count too low" && exit 1)
          [ "$SECTION_COUNT" -ge 3 ] || (echo "Outline section count too low" && exit 1)
          echo "SRT kit valid"

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: docker-e2e-results
          path: test-workspace/
          retention-days: 7

  # LLM-based semantic evaluation using Claude Code Action (manual trigger only)
  semantic-evaluation:
    runs-on: ubuntu-latest
    needs: docker-e2e
    if: github.event_name == 'workflow_dispatch'
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: docker-e2e-results
          path: test-workspace/

      - name: Evaluate pipeline outputs with Claude Code
        uses: anthropics/claude-code-action@v1
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          prompt: |
            You are an evaluator for a content intelligence pipeline. Your task is to assess the quality of each step's output against the original source content.

            ## Step 1: Read the Source Content
            First, find and read the original content file:
            - Look in test-workspace/contentItem/*/content.md (there should be one session folder)
            - This is the ground truth for all evaluations

            ## Step 2: Evaluate Summary (test-workspace/contentItem/*/summary.json)
            Compare against content.md and score using additive criteria (1 point each, max 4):
            - **Accuracy** (1 pt): Key facts match the source without fabrication
            - **Completeness** (1 pt): Main themes from each section are captured
            - **Clarity** (1 pt): Headline and TLDR are well-written
            - **Faithfulness** (1 pt): No hallucinated information beyond source

            ## Step 3: Evaluate Ideas (test-workspace/contentItem/*/ideas.json)
            Score using additive criteria (1 point each, max 4):
            - **Hooks** (1 pt): At least 3 varied, engaging hooks derived from content
            - **Angles** (1 pt): At least 3 unique perspectives relevant to source themes
            - **Questions** (1 pt): Thought-provoking questions that extend the source material
            - **Grounding** (1 pt): Ideas are traceable to source content

            ## Step 4: Evaluate Outline (test-workspace/contentItem/*/outline.json)
            Score using additive criteria (1 point each, max 4):
            - **Structure** (1 pt): Logical flow with clear sections
            - **Coverage** (1 pt): Addresses main themes from source
            - **Actionable** (1 pt): Notes provide useful guidance for writing
            - **Realistic** (1 pt): Word estimates are reasonable (total 1000-3000 words)

            ## Output Format
            Print your evaluation for each step:

            === SOURCE CONTENT ===
            File: [path to content.md]
            Title: [extracted title]
            Sections: [list main sections]

            === SUMMARY EVALUATION ===
            - Accuracy: [0/1] - [reason]
            - Completeness: [0/1] - [reason]
            - Clarity: [0/1] - [reason]
            - Faithfulness: [0/1] - [reason]
            Score: [X]/4

            === IDEAS EVALUATION ===
            - Hooks: [0/1] - [reason]
            - Angles: [0/1] - [reason]
            - Questions: [0/1] - [reason]
            - Grounding: [0/1] - [reason]
            Score: [X]/4

            === OUTLINE EVALUATION ===
            - Structure: [0/1] - [reason]
            - Coverage: [0/1] - [reason]
            - Actionable: [0/1] - [reason]
            - Realistic: [0/1] - [reason]
            Score: [X]/4

            === FINAL RESULT ===
            Total: [X]/12
            Status: [PASS if >= 9, FAIL if < 9]

            If FAIL, use Bash to exit with code 1.
